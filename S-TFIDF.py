#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2017/8/15 15:30
# @Author  : ELP
# @Site    :
# @File    : Task1_S-TFIDF.py
# @Software: PyCharm

import csv
import re
# import datetime as dt
# reload(sys)
# sys.setdefaultencoding('utf-8')
import time

import jieba.analyse

'''
To run this python file, please:
1.install jieba 0.38; load python 2.7
2.add path of text data: in line 54
-----------------------------------------------
Time(test data): it takes about 3000s~3300s to run off this S-TFIDF model on Mac OS Sierra 10.12.6
                 (2.4 GHz Intel Core i5; 8 GB 1600 MHz DDR3)
'''

start = time.process_time()
flag = 1

# Load self-defined mian dictionary
jieba.load_userdict('dicts/dictionary.txt')

# Load training data and extract all answers unrepeatablely
file_answer = 'data/train_task_1/SMPCUP2017_TrainingData_Task1.txt'
f_answer = list(open(file_answer, encoding="utf-8").readlines())
blog_id_answer = [s.strip().split('\001')[0] for s in f_answer]
key_words1 = [s.strip().split('\001')[1] for s in f_answer]
key_words2 = [s.strip().split('\001')[2] for s in f_answer]
key_words3 = [s.strip().split('\001')[3] for s in f_answer]
key_words4 = [s.strip().split('\001')[4] for s in f_answer]
key_words5 = [s.strip().split('\001')[5] for s in f_answer]
keywords = []
keywords.extend(key_words1)
keywords.extend(key_words2)
keywords.extend(key_words3)
keywords.extend(key_words4)
keywords.extend(key_words5)
keywords1 = set(keywords)

# load test data
file_answer_test = 'data/test_set/SMPCUP2017_TestSet_Task1.txt'
f_answer_test = list(open(file_answer_test, "r").readlines())
blog_id_test = [s.strip().split('\001')[0] for s in f_answer_test]

# load text data
contentfile = 'F:/Final/SMPCUP2017数据集/1_BlogContent.txt'

# load self-defined stopwords dictionary
cis = []
for ci in open("dicts/stopwords (2).txt", encoding="utf-8"):
    cis.append(ci.strip())

# load self-defined idf dictionary
dic = {}
for line in open("dicts/idf_dictionary.txt", encoding="utf-8").readlines():
    dic[line.split(' ')[0]] = line.split(' ')[1]

# load self-defined 42topic-top100-words dictionary,which is generated by LDA
dicc = {}
for line in open("dicts/42topicTo100words.txt", encoding="utf-8"):
    L = list(line.strip().split(" "))
    for l in L:
        if dicc.get(l) == None:
            dicc[l] = 1
        else:
            dicc[l] = dicc[l] + 1

# store results in a CSV file
currenttime = str(time.time())
csvname = "res/task1_S-TFIDF" + "-" + currenttime + ".csv"
csvfile = open(csvname, mode="a")
print(csvname)
writer = csv.writer(csvfile)
writer.writerow(['contentid', 'keyword1', 'keyword2', ',keyword3'])


# weight adjustment for TopN keywords selected by Jieba
def words_process(words):
    for word in words:
        # weight adjustment for Chinese words and phrases if they are in self-defined idf dictionary
        # 如果idf词典中该词不为空，且该词为中文
        if dic.get(word) != None and re.match(u"[\u4e00-\u9fa5]+", word):
            # 该词的权重为
            words[word] = float(jieba.lcut(eachline).count(word)) * float(dic[word])
        # weight adjustment for phrases and words containg english
        if re.match("[a-z]+", word):
            words[word] *= 1.7
    for key in words:
        # weigth adjustment for words and phrases if they are in self-defined 42topic-top100-words dictionary
        if dicc.get(key) != None:
            '''
            the formula "(1.0 / float(dicc[key]) - 1.0 / 42.0)" was drawn lessons from the fifth winner of CCF contest:
            https://github.com/coderSkyChen/2016CCF_BDCI_Sougou
            '''
            words[key] = words[key] * (1.0 / float(dicc[key]) - 1.0 / 42.0)
        else:
            words[key] = words[key]
    return sorted(words.items(), key=lambda d: d[1], reverse=True)


for eachline in open(contentfile, encoding="utf-8"):
    # 对于在数据集中的每一行
    # flag=flag+1
    # if flag==110:
    #     break
    # blog_id对应数据集中的UD
    blog_id = eachline.split('\001')[0]
    # 如果数据集中的UD和测试集中的相对应
    if blog_id == 'D0002044':
        # print(flag / 1000000)
        blog_title = eachline.split('\001')[1].lower()
        blog_content = eachline.split('\001')[2].lower()

        # add extra weight for the title of the document
        # 将标题重复12次后和文章内容结合，以提高权重
        material = (blog_title + " ") * 12 + blog_content

        # extract all english words entirely from thedocument and temporarily add them into main dictionary
        # 筛选所有英文单词和数字
        L = set(re.findall('[a-z0-9]+', blog_title + " " + blog_content))
        print(L)
        print(material)
        n = set()
        for m in L:
            # 如果不是数字
            if m.isdigit() != True:
                # 将该词加入jieba词典
                jieba.add_word(m)
                # 动态调节单词词频
                jieba.suggest_freq(m, True)
                n.add(m)

        s = []
        words = {}
        # 结巴提取top20的关键字，权重，名词
        for x, weight in jieba.analyse.extract_tags(material, topK=20, withWeight=True, allowPOS=('n')):
            # 如果x不在停止词词典里，且x不是数字
            if (x not in cis) and (x.isdigit() == False):
                # weight adjustment for keywords according to their length
                # print(weight)
                # print(len(x))
                # 根据长度来加权
                words[x] = weight * float(len(x))
        i = 0
        for word in words_process(words):
            s.append(word[0])
            if i >= 14:
                break
            i += 1

        r = []
        words1 = {}
        for x1, weight1 in jieba.analyse.textrank(material, topK=20, withWeight=True, allowPOS=('n')):
            if (x1 not in cis) and (x1.isdigit() == False):
                words1[x1] = weight1 * float(len(x1))
        j = 0
        for word1 in words_process(words1):
            r.append(word1[0])
            if j >= 14:
                break
            j += 1

        # preserve words or phrases that are both extracted by textrank and tfidf and used to selected in train keywords
        u = []
        u.append(blog_id)
        for key in s:
            if key in r and key in keywords:
                u.append(key)

        if len(u) < 5:
            # if the keywords of the document are less than 3, get new words orderly in the selection of promoted tfidf model
            words3 = {}
            for x3, weight3 in jieba.analyse.extract_tags(material, withWeight=True, topK=5, allowPOS=('n')):
                if (x3 not in cis) and (x3.isdigit() == False) and (x3 not in u):
                    words3[x3] = weight3 * float(len(x3))
            for word3 in words_process(words3):
                u.append(word3[0])
                if len(u) >= 5:
                    break

        # return top3 keywords
        writer.writerow(u[:4])
        # print("true")
        # delete all temporarily added words and phrases in line 115-121
        for key in n:
            jieba.del_word(key)
    # else:
    #     print("false")
csvfile.close()
end = time.process_time()
print('\n')
print("total: %f s" % (end - start))
